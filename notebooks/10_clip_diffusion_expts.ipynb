{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": { "id": "nI0KmN2gCfYN" },
      "source": [
        "# CLIP-guided Diffusion — Lightweight Prototypes\n",
        "\n",
        "Objective: prototype text-driven image generation and style-transfer using CLIP guidance to inform early product demos.\n",
        "\n",
        "**Contents**\n",
        "1. Environment & safety note\n",
        "2. Text-to-image prototype\n",
        "3. Image-to-image (style transfer) prototype\n",
        "4. Observations & limitations\n"
      ],
      "id": "nI0KmN2gCfYN"
    },
    {
      "cell_type": "code",
      "metadata": { "id": "YewEcPL-CfYO" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %% Environment (comment in the line below on Colab if imports fail)\n",
        "# !pip -q install diffusers==0.30.0 transformers==4.44.2 accelerate==0.34.2 safetensors xformers==0.0.27.post2\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython.display import display\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('DEVICE:', DEVICE, '| CUDA OK:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "    except Exception:\n",
        "        pass"
      ],
      "id": "YewEcPL-CfYO"
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "c51pMRr_CfYP" },
      "source": [
        "### Safety & licensing\n",
        "- Use models in accordance with their licenses and terms.\n",
        "- Add a basic prompt/NSFW filter when exposing a public UI.\n"
      ],
      "id": "c51pMRr_CfYP"
    },
    {
      "cell_type": "code",
      "metadata": { "id": "saveShowCell" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %% Helpers: save + display\n",
        "OUTDIR = Path('/content/outputs'); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_and_show(img: Image.Image, prefix: str = 'gen') -> str:\n",
        "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    path = OUTDIR / f\"{prefix}_{ts}.png\"\n",
        "    img.save(path)\n",
        "    display(img)\n",
        "    print('Saved →', path)\n",
        "    return str(path)"
      ],
      "id": "saveShowCell"
    },
    {
      "cell_type": "code",
      "metadata": { "id": "FXZPjUPTCfYP" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %% Text-to-image (Stable Diffusion v1.5) — cached pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "_TXT2IMG = None\n",
        "\n",
        "def get_txt2img():\n",
        "    global _TXT2IMG\n",
        "    if _TXT2IMG is None:\n",
        "        kw = {}\n",
        "        if DEVICE == 'cuda':\n",
        "            kw['torch_dtype'] = torch.float16\n",
        "        _TXT2IMG = StableDiffusionPipeline.from_pretrained(\n",
        "            'runwayml/stable-diffusion-v1-5', **kw\n",
        "        ).to(DEVICE)\n",
        "        if DEVICE == 'cuda':\n",
        "            try:\n",
        "                _TXT2IMG.enable_xformers_memory_efficient_attention()\n",
        "            except Exception:\n",
        "                pass\n",
        "    return _TXT2IMG\n",
        "\n",
        "def generate(prompt: str, steps: int = 35, guidance: float = 7.5, seed: Optional[int] = 1234,\n",
        "             negative: str = 'nsfw, nude, watermark, text, logo') -> Image.Image:\n",
        "    pipe = get_txt2img()\n",
        "    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed is not None else None\n",
        "    out = pipe(prompt, num_inference_steps=steps, guidance_scale=guidance,\n",
        "               negative_prompt=negative, generator=g)\n",
        "    return out.images[0]\n",
        "\n",
        "# Demo run (enabled)\n",
        "DO_RUN_T2I = True\n",
        "if DO_RUN_T2I:\n",
        "    img = generate('product mockup, clean studio lighting, minimal background, 50mm photo')\n",
        "    save_and_show(img, prefix='t2i')"
      ],
      "id": "FXZPjUPTCfYP"
    },
    {
      "cell_type": "code",
      "metadata": { "id": "WlKyLmNpCfYQ" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %% Image-to-image (style nudging) — cached pipeline\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "_I2I = None\n",
        "\n",
        "def get_img2img():\n",
        "    global _I2I\n",
        "    if _I2I is None:\n",
        "        kw = {}\n",
        "        if DEVICE == 'cuda':\n",
        "            kw['torch_dtype'] = torch.float16\n",
        "        _I2I = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "            'runwayml/stable-diffusion-v1-5', **kw\n",
        "        ).to(DEVICE)\n",
        "        if DEVICE == 'cuda':\n",
        "            try:\n",
        "                _I2I.enable_xformers_memory_efficient_attention()\n",
        "            except Exception:\n",
        "                pass\n",
        "    return _I2I\n",
        "\n",
        "def stylize(init_img: Image.Image, prompt: str, strength: float = 0.6, steps: int = 35,\n",
        "            guidance: float = 7.5, seed: Optional[int] = 1234) -> Image.Image:\n",
        "    pipe = get_img2img()\n",
        "    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed is not None else None\n",
        "    out = pipe(prompt=prompt, image=init_img, strength=strength,\n",
        "               num_inference_steps=steps, guidance_scale=guidance, generator=g)\n",
        "    return out.images[0]\n",
        "\n",
        "# Demo run (enabled)\n",
        "DO_RUN_I2I = True\n",
        "if DO_RUN_I2I:\n",
        "    base = Image.new('RGB', (512, 512), 'white')\n",
        "    out = stylize(base, 'isometric icon style, soft gradients, crisp edges')\n",
        "    save_and_show(out, prefix='i2i')"
      ],
      "id": "WlKyLmNpCfYQ"
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "rPrAo9zrCfYQ" },
      "source": [
        "## Observations\n",
        "- For quick demos, a single A10G/T4 is adequate; batch size 1, 512x512.\n",
        "- Prompt engineering matters more than step counts for early concept checks.\n",
        "- For consistent branding, keep a small prompt library checked into the repo.\n",
        "\n",
        "## Next steps\n",
        "- Wire a basic Streamlit UI to collect prompts and seed; export PNGs.\n",
        "- If going beyond prototype, consider ControlNet or fine-tuning for product styles.\n"
      ],
      "id": "rPrAo9zrCfYQ"
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.11" },
    "colab": { "provenance": [], "toc_visible": true }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wav2Vec2 Speech-to-Text â€” Feasibility & Baseline (SageMaker)\n",
        "\n",
        "Objective: establish a lean, repeatable path to an **ASR endpoint on SageMaker** using a Hugging Face Wav2Vec2 model; validate basic **latency**, **cost posture**, and **quality** (WER/CER) on a small test set.\n",
        "\n",
        "**Scope**\n",
        "- No training; inference-only via HF DLC or minimal handler.\n",
        "- Tiny sample set to sanity-check performance before productizing.\n",
        "\n",
        "**Contents**\n",
        "1. Environment & config\n",
        "2. Local smoke test\n",
        "3. Endpoint invocation helper\n",
        "4. Optional evaluation (WER/CER) if references available\n",
        "5. Notes & follow-ups\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {}
      },
      "outputs": [],
      "source": [
        "# %% Environment (run locally). Comment-out installs if already present.\n",
        "# !pip install --quiet boto3 jiwer soundfile pydub\n",
        "import os, io, json, time, pathlib\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from jiwer import wer, cer\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Config\n",
        "AWS_REGION = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
        "ENDPOINT_NAME = os.environ.get(\"SM_ENDPOINT_NAME\", \"vrynt-stt-demo\")\n",
        "SAMPLE_DIR = pathlib.Path(\"samples\")  # put a few short wav/mp3 files here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Local smoke test (no endpoint)\n",
        "For a pure local check with a small model, use `transformers` (optional). This cell is intentionally skipped to keep the notebook lightweight.\n",
        "\n",
        "> If you want to run locally, install `transformers` + `datasets` + `torch` and load `facebook/wav2vec2-base-960h`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {}
      },
      "outputs": [],
      "source": [
        "SKIP_LOCAL = True  # set to False if you want to run a local small-model smoke test\n",
        "if not SKIP_LOCAL:\n",
        "    # from transformers import AutoProcessor, AutoModelForCTC\n",
        "    # import torch, soundfile as sf\n",
        "    # processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    # model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    # waveform, sr = sf.read(\"samples/short.wav\")\n",
        "    # inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    # with torch.inference_mode():\n",
        "    #     ids = model(**inputs).logits.argmax(-1)\n",
        "    # print(processor.batch_decode(ids)[0])\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Invoke SageMaker endpoint\n",
        "Helper to send audio bytes to the endpoint and get a transcript JSON back."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {}
      },
      "outputs": [],
      "source": [
        "def invoke_stt(endpoint_name: str, region: str, audio_bytes: bytes):\n",
        "    smr = boto3.client(\"sagemaker-runtime\", region_name=region, config=Config(retries={'max_attempts': 3}))\n",
        "    t0 = time.time()\n",
        "    resp = smr.invoke_endpoint(EndpointName=endpoint_name, ContentType=\"application/x-audio\", Body=audio_bytes)\n",
        "    latency = time.time() - t0\n",
        "    body = resp[\"Body\"].read()\n",
        "    try:\n",
        "        data = json.loads(body.decode(\"utf-8\"))\n",
        "    except Exception:\n",
        "        data = {\"raw\": body.decode(\"utf-8\", errors=\"ignore\")}\n",
        "    return data, latency\n",
        "\n",
        "def load_audio_bytes(path: pathlib.Path) -> bytes:\n",
        "    audio = AudioSegment.from_file(path, format=path.suffix.strip('.'))\n",
        "    buf = io.BytesIO()\n",
        "    audio.export(buf, format=\"wav\")\n",
        "    return buf.getvalue()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch over a small sample folder\n",
        "If you add files under `samples/` (e.g., `short1.wav`, `short2.mp3`), this loop will collect latency stats.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "if SAMPLE_DIR.exists():\n",
        "    for p in sorted(SAMPLE_DIR.iterdir()):\n",
        "        if p.suffix.lower() not in {'.wav', '.mp3'}:\n",
        "            continue\n",
        "        data, lat = invoke_stt(ENDPOINT_NAME, AWS_REGION, load_audio_bytes(p))\n",
        "        results.append({\"file\": p.name, \"latency_s\": round(lat, 3), \"text\": data.get(\"text\") or data})\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Optional: quality evaluation (WER/CER)\n",
        "If you have ground-truth references, place a `references.jsonl` in `samples/` with lines:\n",
        "\n",
        "```json\n",
        "{\"file\": \"short1.wav\", \"reference\": \"hello world\"}\n",
        "```\n",
        "\n",
        "This cell will join predictions with references and compute WER/CER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "pred_map = {r['file']: r for r in results}\n",
        "refs_path = SAMPLE_DIR / 'references.jsonl'\n",
        "if refs_path.exists():\n",
        "    refs = [json.loads(l) for l in open(refs_path)]\n",
        "    y_true, y_pred = [], []\n",
        "    for r in refs:\n",
        "        file = r['file']\n",
        "        gt = r['reference']\n",
        "        pred = pred_map.get(file, {}).get('text', '') if pred_map else ''\n",
        "        if isinstance(pred, dict):\n",
        "            pred = pred.get('text', '')\n",
        "        y_true.append(gt)\n",
        "        y_pred.append(pred)\n",
        "    print({\n",
        "        'samples': len(y_true),\n",
        "        'wer': wer(y_true, y_pred),\n",
        "        'cer': cer(y_true, y_pred)\n",
        "    })\n",
        "else:\n",
        "    print('No references.jsonl found; skipping WER/CER.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Notes\n",
        "- For the public demo, start with a single `ml.m5.xlarge` and autoscale when needed.\n",
        "- Keep transcripts out of logs; store only timing + request size if needed.\n",
        "- If latency tails are high on MP3, prefer WAV uploads from the client.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

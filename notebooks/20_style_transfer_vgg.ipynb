{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Style Transfer (VGG19)\n",
        "\n",
        "This notebook reproduces a classic style-transfer baseline using **VGG19** feature maps and an optimization loop.\n",
        "It mirrors the parameters referenced in the legacy notes while using modern, readable PyTorch code.\n",
        "\n",
        "**What you can do**\n",
        "1. Set content and style images.\n",
        "2. Choose layer weights and total variation strength.\n",
        "3. Optimize the output image using LBFGS or Adam.\n",
        "4. Save the result under `outputs/style_*.png`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !pip install torch torchvision pillow\n",
        "import torch, torchvision as tv\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "\n",
        "def load_image(path, size=512):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = tv.transforms.Resize(size)(img)\n",
        "    img = tv.transforms.ToTensor()(img).unsqueeze(0)\n",
        "    return img.to(device)\n",
        "\n",
        "def tensor_to_pil(t):\n",
        "    t = t.detach().clamp(0,1).cpu().squeeze(0)\n",
        "    return tv.transforms.ToPILImage()(t)\n",
        "\n",
        "outdir = Path('outputs'); outdir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config\n",
        "Set your content and style images. For a quick test, point both to the same image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONTENT = 'path/to/content.jpg'   # <- change\n",
        "STYLE   = 'path/to/style.jpg'     # <- change\n",
        "SIZE    = 512\n",
        "STEPS   = 300\n",
        "TV_WEIGHT = 1e-5\n",
        "STYLE_WEIGHT = 1e4\n",
        "CONTENT_WEIGHT = 1.0\n",
        "LR = 0.03\n",
        "USE_LBFGS = False  # LBFGS often converges faster but uses more memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and loss setup\n",
        "We use VGG19 features; style loss is Gram matrix MSE across chosen layers, content loss is MSE at a mid-level conv layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vgg = tv.models.vgg19(weights=tv.models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "for p in vgg.parameters(): p.requires_grad_(False)\n",
        "\n",
        "content_layers = ['21']  # relu4_2\n",
        "style_layers = ['0','5','10','19','28']  # relu1_1 ... relu5_1\n",
        "\n",
        "def gram_matrix(x):\n",
        "    b, c, h, w = x.size()\n",
        "    f = x.view(b*c, h*w)\n",
        "    G = f @ f.t() / (c*h*w)\n",
        "    return G\n",
        "\n",
        "def get_features(x):\n",
        "    feats = {}\n",
        "    h = x\n",
        "    for i, layer in enumerate(vgg):\n",
        "        h = layer(h)\n",
        "        if str(i) in set(content_layers + style_layers):\n",
        "            feats[str(i)] = h\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "content = load_image(CONTENT, SIZE)\n",
        "style = load_image(STYLE, SIZE)\n",
        "\n",
        "target = content.clone().requires_grad_(True)\n",
        "content_targets = get_features(content)\n",
        "style_targets = {k: gram_matrix(v) for k,v in get_features(style).items() if k in style_layers}\n",
        "\n",
        "opt = optim.LBFGS([target]) if USE_LBFGS else optim.Adam([target], lr=LR)\n",
        "\n",
        "def total_variation(x):\n",
        "    return ((x[:,:,:-1,:]-x[:,:,1:,:]).abs().mean() + (x[:,:,:,:-1]-x[:,:,:,1:]).abs().mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize\n",
        "This runs for ~300 steps by default. Adjust `STEPS` or switch to LBFGS to speed up.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "step = 0\n",
        "t0 = time.time()\n",
        "while step < STEPS:\n",
        "    def closure():\n",
        "        opt.zero_grad()\n",
        "        feats = get_features(target)\n",
        "        # Content loss\n",
        "        c_loss = 0.0\n",
        "        for l in content_layers:\n",
        "            c_loss = c_loss + (feats[l] - content_targets[l]).pow(2).mean()\n",
        "        # Style loss\n",
        "        s_loss = 0.0\n",
        "        for l in style_layers:\n",
        "            s_loss = s_loss + (gram_matrix(feats[l]) - style_targets[l]).pow(2).mean()\n",
        "        tv_loss = total_variation(target) * TV_WEIGHT\n",
        "        loss = CONTENT_WEIGHT*c_loss + STYLE_WEIGHT*s_loss + tv_loss\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    if isinstance(opt, optim.LBFGS):\n",
        "        loss = opt.step(closure)\n",
        "    else:\n",
        "        loss = closure()\n",
        "        opt.step()\n",
        "    step += 1\n",
        "    if step % 50 == 0:\n",
        "        print(f\"step {step}/{STEPS} loss={loss.item():.4f}\")\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"done in {elapsed:.1f}s\")\n",
        "out_path = outdir / f\"style_{int(time.time())}.png\"\n",
        "tensor_to_pil(target).save(out_path)\n",
        "print(\"saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- To reproduce a demo quickly, keep size at 512 and switch to LBFGS (`USE_LBFGS=True`).\n",
        "- Adjust `STYLE_WEIGHT` up for stronger stylization; `TV_WEIGHT` reduces noise and keeps edges clean.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
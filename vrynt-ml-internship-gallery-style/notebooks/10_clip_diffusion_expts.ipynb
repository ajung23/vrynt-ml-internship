{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP-guided Diffusion â€” Lightweight Prototypes\n",
        "\n",
        "Objective: prototype text-driven image generation and style-transfer using CLIP guidance to inform early product demos.\n",
        "\n",
        "**Contents**\n",
        "1. Environment & safety note\n",
        "2. Text-to-image prototype\n",
        "3. Image-to-image (style transfer) prototype\n",
        "4. Observations & limitations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% Environment (comment out installs if pre-installed)\n",
        "# !pip install --quiet diffusers transformers accelerate torch safetensors pillow\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import torch\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Safety & licensing\n",
        "- Use models in accordance with their licenses and terms.\n",
        "- Add a basic prompt/NSFW filter when exposing a public UI.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% Text-to-image (Stable Diffusion v1.5 example)\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "def generate(prompt: str, steps: int = 30, guidance: float = 7.5, seed: Optional[int] = 1234):\n",
        "    pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')\n",
        "    pipe = pipe.to(DEVICE)\n",
        "    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed is not None else None\n",
        "    out = pipe(prompt, num_inference_steps=steps, guidance_scale=guidance, generator=g)\n",
        "    return out.images[0]\n",
        "\n",
        "# Example (skip execution in CI):\n",
        "DO_RUN = False\n",
        "if DO_RUN:\n",
        "    img = generate('product mockup, clean studio lighting, minimal background')\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% Image-to-image (style nudging)\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "def stylize(init_img: Image.Image, prompt: str, strength: float = 0.6, steps: int = 30, seed: Optional[int] = 1234):\n",
        "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')\n",
        "    pipe = pipe.to(DEVICE)\n",
        "    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed is not None else None\n",
        "    out = pipe(prompt=prompt, image=init_img, strength=strength, num_inference_steps=steps, generator=g)\n",
        "    return out.images[0]\n",
        "\n",
        "# Example (skip execution):\n",
        "DO_RUN = False\n",
        "if DO_RUN:\n",
        "    base = Image.new('RGB', (512,512), 'white')\n",
        "    out = stylize(base, 'isometric icon style, soft gradients, crisp edges')\n",
        "    display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations\n",
        "- For quick demos, a single A10G/T4 is adequate; batch size 1, 512x512.\n",
        "- Prompt engineering matters more than step counts for early concept checks.\n",
        "- For consistent branding, keep a small prompt library checked into the repo.\n",
        "\n",
        "## Next steps\n",
        "- Wire a basic Streamlit UI to collect prompts and seed; export PNGs.\n",
        "- If going beyond prototype, consider ControlNet or fine-tuning for product styles.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}